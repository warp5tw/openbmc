From 96095a43e29e5275cbaf4c4a65b6da8fcaab06b5 Mon Sep 17 00:00:00 2001
From: Joseph Liu <kwliu@nuvoton.com>
Date: Tue, 18 Feb 2020 15:42:31 +0800
Subject: [PATCH] driver: misc: add nuvoton vdmx/vdma driver

Signed-off-by: Joseph Liu <kwliu@nuvoton.com>
---
 arch/arm/boot/dts/nuvoton-common-npcm7xx.dtsi |    3 +-
 .../dts/nuvoton-npcm750-runbmc-olympus.dts    |    5 +
 drivers/misc/Kconfig                          |    7 +
 drivers/misc/Makefile                         |    1 +
 drivers/misc/aspeed_mctp.c                    | 1126 +++++++++++++++++
 drivers/misc/npcm-vdm/Kconfig                 |    2 +-
 drivers/misc/npcm7xx-pci-vdm.c                |  781 ++++++++++++
 7 files changed, 1923 insertions(+), 2 deletions(-)
 create mode 100644 drivers/misc/aspeed_mctp.c
 create mode 100644 drivers/misc/npcm7xx-pci-vdm.c

diff --git a/arch/arm/boot/dts/nuvoton-common-npcm7xx.dtsi b/arch/arm/boot/dts/nuvoton-common-npcm7xx.dtsi
index ae5526880b60..ade714473127 100644
--- a/arch/arm/boot/dts/nuvoton-common-npcm7xx.dtsi
+++ b/arch/arm/boot/dts/nuvoton-common-npcm7xx.dtsi
@@ -234,7 +234,8 @@
 			compatible = "nuvoton,npcm750-vdm";
 			reg = <0xe0800000 0x1000
 				   0xf0822000 0x1000>;
-			interrupts = <GIC_SPI 29 IRQ_TYPE_LEVEL_HIGH>;
+			interrupts = <GIC_SPI 30 IRQ_TYPE_LEVEL_HIGH>,
+				<GIC_SPI 29 IRQ_TYPE_LEVEL_HIGH>;
 		};
 
 		fiu0: fiu@fb000000 {
diff --git a/arch/arm/boot/dts/nuvoton-npcm750-runbmc-olympus.dts b/arch/arm/boot/dts/nuvoton-npcm750-runbmc-olympus.dts
index 68449d1d0623..9a6107f08acd 100644
--- a/arch/arm/boot/dts/nuvoton-npcm750-runbmc-olympus.dts
+++ b/arch/arm/boot/dts/nuvoton-npcm750-runbmc-olympus.dts
@@ -28,6 +28,7 @@
 		udc8 = &udc8;
 		udc9 = &udc9;
 		emmc0 = &sdhci0;
+		vdma = &vdma;
 		i2c0 = &i2c0;
 		i2c1 = &i2c1;
 		i2c2 = &i2c2;
@@ -258,6 +259,10 @@
 			status = "okay";
 		};
 
+		vdma: vdma@e0800000 {
+			status = "okay";
+		};
+
 		pcimbox: pcimbox@f0848000 {
 			status = "okay";
 		};
diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index a7f7d6bd7ed5..f4fb461d39c4 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -493,6 +493,13 @@ config NPCM7XX_MCU_FLASH
 	help
 	  Control GPIO to transmit SPI signals to support MCU firmware flash function.
 
+config NPCM7XX_PCI_VDM
+	tristate "NPCM7xx PCI VDM/VDMA Controller"
+	depends on (ARCH_NPCM7XX || COMPILE_TEST) && REGMAP && MFD_SYSCON
+	help
+	  Expose the NPCM750/730/715/705 PCI VDM/VDMA registers found on
+	  Nuvoton SOCs to userspace.
+
 source "drivers/misc/c2port/Kconfig"
 source "drivers/misc/eeprom/Kconfig"
 source "drivers/misc/cb710/Kconfig"
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index c5bba1c11df8..7fd7f61444cf 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -59,6 +59,7 @@ obj-$(CONFIG_HABANA_AI)		+= habanalabs/
 obj-$(CONFIG_XILINX_SDFEC)	+= xilinx_sdfec.o
 obj-$(CONFIG_NPCM7XX_LPC_BPC)	+= npcm7xx-lpc-bpc.o
 obj-$(CONFIG_NPCM7XX_PCI_MBOX)	+= npcm7xx-pci-mbox.o
+obj-$(CONFIG_NPCM7XX_PCI_VDM)  += npcm7xx-pci-vdm.o
 obj-$(CONFIG_NPCM_VDM)		+= npcm-vdm/
 obj-$(CONFIG_NPCM7XX_JTAG_MASTER)	+= npcm7xx-jtag-master.o
 obj-$(CONFIG_NPCM7XX_MCU_FLASH)		+= npcm7xx-mcu-flash.o
diff --git a/drivers/misc/aspeed_mctp.c b/drivers/misc/aspeed_mctp.c
new file mode 100644
index 000000000000..2b942242419b
--- /dev/null
+++ b/drivers/misc/aspeed_mctp.c
@@ -0,0 +1,1126 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (c) 2020, Intel Corporation.
+
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mfd/syscon.h>
+#include <linux/miscdevice.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/pci.h>
+#include <linux/poll.h>
+#include <linux/ptr_ring.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/workqueue.h>
+#include <uapi/linux/aspeed-mctp.h>
+
+/* AST2600 MCTP Controller registers */
+#define ASPEED_MCTP_CTRL	0x000
+#define  TX_CMD_TRIGGER		BIT(0)
+#define  RX_CMD_READY		BIT(4)
+#define  MATCHING_EID		BIT(9)
+
+#define ASPEED_MCTP_TX_CMD	0x004
+#define ASPEED_MCTP_RX_CMD	0x008
+
+#define ASPEED_MCTP_INT_STS	0x00c
+#define ASPEED_MCTP_INT_EN	0x010
+#define  TX_CMD_SENT_INT	BIT(0)
+#define  TX_CMD_LAST_INT	BIT(1)
+#define  TX_CMD_WRONG_INT	BIT(2)
+#define  RX_CMD_RECEIVE_INT	BIT(8)
+#define  RX_CMD_NO_MORE_INT	BIT(9)
+
+#define ASPEED_MCTP_EID		0x014
+#define ASPEED_MCTP_OBFF_CTRL	0x018
+
+#define ASPEED_MCTP_ENGINE_CTRL		0x01c
+#define  TX_MAX_PAYLOAD_SIZE_SHIFT	0
+#define  TX_MAX_PAYLOAD_SIZE_MASK	GENMASK(1, TX_MAX_PAYLOAD_SIZE_SHIFT)
+#define  TX_MAX_PAYLOAD_SIZE(x) \
+	(((x) << TX_MAX_PAYLOAD_SIZE_SHIFT) & TX_MAX_PAYLOAD_SIZE_MASK)
+#define  RX_MAX_PAYLOAD_SIZE_SHIFT	4
+#define  RX_MAX_PAYLOAD_SIZE_MASK	GENMASK(5, RX_MAX_PAYLOAD_SIZE_SHIFT)
+#define  RX_MAX_PAYLOAD_SIZE(x) \
+	(((x) << RX_MAX_PAYLOAD_SIZE_SHIFT) & RX_MAX_PAYLOAD_SIZE_MASK)
+#define FIFO_LAYOUT_SHIFT		8
+#define FIFO_LAYOUT_MASK		GENMASK(9, FIFO_LAYOUT_SHIFT)
+#define FIFO_LAYOUT(x) \
+	(((x) << FIFO_LAYOUT_SHIFT) & FIFO_LAYOUT_MASK)
+
+#define ASPEED_MCTP_RX_BUF_ADDR		0x020
+#define ASPEED_MCTP_RX_BUF_SIZE		0x024
+#define ASPEED_MCTP_RX_BUF_RD_PTR	0x028
+#define  UPDATE_RX_RD_PTR		BIT(31)
+#define  RX_BUFFER_RD_PTR		GENMASK(11, 0)
+#define ASPEED_MCTP_RX_BUF_WR_PTR	0x02c
+#define  RX_BUFFER_WR_PTR		GENMASK(11, 0)
+
+#define ASPEED_MCTP_TX_BUF_ADDR		0x004
+#define ASPEED_MCTP_TX_BUF_SIZE		0x034
+#define ASPEED_MCTP_TX_BUF_RD_PTR	0x038
+#define  UPDATE_TX_RD_PTR		BIT(31)
+#define  TX_BUFFER_RD_PTR		GENMASK(11, 0)
+#define ASPEED_MCTP_TX_BUF_WR_PTR	0x03c
+#define  TX_BUFFER_WR_PTR		GENMASK(11, 0)
+
+#define ADDR_LEN	(BIT(26) - 1)
+#define DATA_ADDR(x)	(((x) >> 4) & ADDR_LEN)
+
+/* TX command */
+#define TX_LAST_CMD		BIT(31)
+#define TX_DATA_ADDR_SHIFT	4
+#define TX_DATA_ADDR_MASK	GENMASK(30, TX_DATA_ADDR_SHIFT)
+#define TX_DATA_ADDR(x) \
+	((DATA_ADDR(x) << TX_DATA_ADDR_SHIFT) & TX_DATA_ADDR_MASK)
+#define TX_RESERVED_1_MASK	GENMASK(1, 0) /* must be 1 */
+#define TX_RESERVED_1		1
+#define TX_STOP_AFTER_CMD	BIT(16)
+#define TX_INTERRUPT_AFTER_CMD	BIT(15)
+#define TX_PACKET_SIZE_SHIFT	2
+#define TX_PACKET_SIZE_MASK	GENMASK(12, TX_PACKET_SIZE_SHIFT)
+#define TX_PACKET_SIZE(x) \
+	(((x) << TX_PACKET_SIZE_SHIFT) & TX_PACKET_SIZE_MASK)
+#define TX_RESERVED_0_MASK	GENMASK(1, 0) /* MBZ */
+#define TX_RESERVED_0		0
+
+/* RX command */
+#define RX_INTERRUPT_AFTER_CMD	BIT(2)
+#define RX_DATA_ADDR_SHIFT	4
+#define RX_DATA_ADDR_MASK	GENMASK(30, RX_DATA_ADDR_SHIFT)
+#define RX_DATA_ADDR(x) \
+	((DATA_ADDR(x) << RX_DATA_ADDR_SHIFT) & RX_DATA_ADDR_MASK)
+
+/* Buffer sizes */
+#define TX_CMD_COUNT 4
+#define RX_CMD_COUNT 4
+#define TX_MAX_CMD_COUNT SZ_4K
+#define RX_MAX_CMD_COUNT SZ_4K
+
+/* PCIe Host Controller registers */
+#define ASPEED_PCIE_MISC_STS_1 0x0c4
+
+/* PCI address definitions */
+#define PCI_DEV_NUM_MASK	GENMASK(4, 0)
+#define PCI_BUS_NUM_SHIFT	5
+#define PCI_BUS_NUM_MASK	GENMASK(12, PCI_BUS_NUM_SHIFT)
+#define GET_PCI_DEV_NUM(x)	((x) & PCI_DEV_NUM_MASK)
+#define GET_PCI_BUS_NUM(x)	(((x) & PCI_BUS_NUM_MASK) >> PCI_BUS_NUM_SHIFT)
+
+/* FIXME: ast2600 supports variable max transmission unit */
+#define ASPEED_MCTP_MTU 64
+
+struct mctp_pcie_packet {
+	struct {
+		u32 hdr[4];
+		u32 payload[16];
+	} data;
+	u32 size;
+};
+
+struct aspeed_mctp_tx_cmd {
+	u32 tx_lo;
+	u32 tx_hi;
+};
+
+struct mctp_buffer {
+	void *vaddr;
+	dma_addr_t dma_handle;
+};
+
+struct mctp_channel {
+	struct mctp_buffer data;
+	struct mctp_buffer cmd;
+	struct tasklet_struct tasklet;
+	u32 rd_ptr;
+	u32 wr_ptr;
+};
+
+struct aspeed_mctp {
+	struct device *dev;
+	struct regmap *map;
+	struct reset_control *reset;
+	struct mctp_channel tx;
+	struct mctp_channel rx;
+	struct list_head clients;
+	spinlock_t clients_lock; /* to protect clients list operations */
+	wait_queue_head_t wait_queue;
+	struct {
+		struct regmap *map;
+		struct delayed_work rst_dwork;
+		bool need_uevent;
+		u16 bdf;
+	} pcie;
+};
+
+struct mctp_client {
+	struct kref ref;
+	struct aspeed_mctp *priv;
+	struct ptr_ring tx_queue;
+	struct ptr_ring rx_queue;
+	struct list_head link;
+	bool disconnected;
+};
+
+#define TX_CMD_BUF_SIZE \
+	PAGE_ALIGN(TX_CMD_COUNT * sizeof(struct aspeed_mctp_tx_cmd))
+#define TX_DATA_BUF_SIZE \
+	 PAGE_ALIGN(TX_CMD_COUNT * sizeof(struct mctp_pcie_packet))
+#define RX_CMD_BUF_SIZE PAGE_ALIGN(RX_CMD_COUNT * sizeof(u32))
+#define RX_DATA_BUF_SIZE \
+	PAGE_ALIGN(RX_CMD_COUNT * sizeof(struct mctp_pcie_packet))
+
+struct kmem_cache *packet_cache;
+
+static void *packet_alloc(gfp_t flags)
+{
+	return kmem_cache_alloc(packet_cache, flags);
+}
+
+static void packet_free(void *packet)
+{
+	kmem_cache_free(packet_cache, packet);
+}
+
+static void aspeed_mctp_rx_trigger(struct mctp_channel *rx)
+{
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+
+	/*
+	 * Even though rx_buf_addr doesn't change, if we don't do the write
+	 * here, the HW doesn't trigger RX.
+	 * Also, note that we're writing 0 as wr_ptr. If we're writing other
+	 * value, the HW behaves in a bizzare way that's hard to explain...
+	 */
+	regmap_write(priv->map, ASPEED_MCTP_RX_BUF_ADDR, rx->cmd.dma_handle);
+	regmap_write(priv->map, ASPEED_MCTP_RX_BUF_WR_PTR, 0);
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, RX_CMD_READY,
+			   RX_CMD_READY);
+}
+
+static void aspeed_mctp_tx_trigger(struct mctp_channel *tx)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+
+	regmap_write(priv->map, ASPEED_MCTP_TX_BUF_ADDR,
+		     tx->cmd.dma_handle);
+	regmap_write(priv->map, ASPEED_MCTP_TX_BUF_WR_PTR, tx->wr_ptr);
+	regmap_update_bits(priv->map, ASPEED_MCTP_CTRL, TX_CMD_TRIGGER,
+			   TX_CMD_TRIGGER);
+}
+
+static void aspeed_mctp_emit_tx_cmd(struct mctp_channel *tx,
+				    struct mctp_pcie_packet *packet, bool last)
+{
+	struct aspeed_mctp_tx_cmd *tx_cmd =
+		(struct aspeed_mctp_tx_cmd *)tx->cmd.vaddr + tx->wr_ptr;
+	u32 packet_sz_dw = packet->size / sizeof(u32) -
+		sizeof(packet->data.hdr) / sizeof(u32);
+	u32 offset = tx->wr_ptr * sizeof(packet->data);
+
+	memcpy(tx->data.vaddr + offset, &packet->data,
+	       sizeof(packet->data));
+
+	tx_cmd->tx_lo |= TX_PACKET_SIZE(packet_sz_dw);
+	tx_cmd->tx_lo |= TX_STOP_AFTER_CMD;
+	tx_cmd->tx_lo |= TX_INTERRUPT_AFTER_CMD;
+	tx_cmd->tx_hi |= TX_RESERVED_1;
+	tx_cmd->tx_hi |= TX_DATA_ADDR(tx->data.dma_handle + offset);
+	if (last)
+		tx_cmd->tx_hi |= TX_LAST_CMD;
+
+	tx->wr_ptr++;
+}
+
+static struct mctp_client *aspeed_mctp_client_alloc(struct aspeed_mctp *priv)
+{
+	struct mctp_client *client;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		goto out;
+
+	kref_init(&client->ref);
+	client->priv = priv;
+	ptr_ring_init(&client->tx_queue, TX_CMD_COUNT, GFP_KERNEL);
+	ptr_ring_init(&client->rx_queue, RX_CMD_COUNT, GFP_ATOMIC);
+
+out:
+	return client;
+}
+
+static void aspeed_mctp_client_free(struct kref *ref)
+{
+	struct mctp_client *client = container_of(ref, typeof(*client), ref);
+
+	ptr_ring_cleanup(&client->rx_queue, &packet_free);
+	ptr_ring_cleanup(&client->tx_queue, &packet_free);
+
+	kfree(client);
+}
+
+static void aspeed_mctp_client_get(struct mctp_client *client)
+{
+	lockdep_assert_held(&client->priv->clients_lock);
+
+	kref_get(&client->ref);
+}
+
+static void aspeed_mctp_client_put(struct mctp_client *client)
+{
+	kref_put(&client->ref, &aspeed_mctp_client_free);
+}
+
+static void aspeed_mctp_tx_tasklet(unsigned long data)
+{
+	struct mctp_channel *tx = (struct mctp_channel *)data;
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+	u32 rd_ptr = READ_ONCE(tx->rd_ptr);
+	struct mctp_pcie_packet *packet;
+	struct mctp_client *client;
+	bool last, trigger = false;
+
+	/* we're called while there's still TX in progress */
+	if (rd_ptr == 0 && tx->wr_ptr != 0)
+		return;
+
+	spin_lock(&priv->clients_lock);
+	client = list_first_entry_or_null(&priv->clients, typeof(*client),
+					  link);
+	if (!client) {
+		spin_unlock(&priv->clients_lock);
+		return;
+	}
+	aspeed_mctp_client_get(client);
+	spin_unlock(&priv->clients_lock);
+
+	/* last tx ended up with buffer size, meaning we now restart from 0 */
+	if (rd_ptr == TX_CMD_COUNT) {
+		WRITE_ONCE(tx->rd_ptr, 0);
+		tx->wr_ptr = 0;
+	}
+
+	while (tx->wr_ptr < TX_CMD_COUNT) {
+		packet = ptr_ring_consume(&client->tx_queue);
+		if (!packet)
+			break;
+
+		last = !__ptr_ring_peek(&client->tx_queue);
+
+		aspeed_mctp_emit_tx_cmd(tx, packet, last);
+		packet_free(packet);
+
+		trigger = true;
+
+		if (last)
+			break;
+	}
+
+	aspeed_mctp_client_put(client);
+
+	if (trigger)
+		aspeed_mctp_tx_trigger(tx);
+}
+
+static void aspeed_mctp_rx_tasklet(unsigned long data)
+{
+	struct mctp_channel *rx = (struct mctp_channel *)data;
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+	u32 rd_ptr = READ_ONCE(priv->rx.rd_ptr);
+	struct mctp_pcie_packet *rx_packet;
+	struct mctp_client *client;
+	int ret;
+
+	spin_lock(&priv->clients_lock);
+	client = list_first_entry_or_null(&priv->clients, typeof(*client),
+					  link);
+	if (!client) {
+		rx->wr_ptr = rd_ptr;
+		spin_unlock(&priv->clients_lock);
+		goto out_skip;
+	}
+	aspeed_mctp_client_get(client);
+	spin_unlock(&priv->clients_lock);
+
+	while (rx->wr_ptr < rd_ptr) {
+		rx_packet = packet_alloc(GFP_ATOMIC);
+		if (!rx_packet) {
+			dev_err(priv->dev, "Failed to allocate RX packet\n");
+			goto out_put;
+		}
+
+		memcpy(&rx_packet->data,
+		       rx->data.vaddr + rx->wr_ptr * sizeof(rx_packet->data),
+		       sizeof(rx_packet->data));
+		rx->wr_ptr++;
+
+		ret = ptr_ring_produce(&client->rx_queue, rx_packet);
+		if (ret) {
+			dev_warn(priv->dev, "Failed to produce RX packet: %d\n",
+				 ret);
+			packet_free(rx_packet);
+			continue;
+		}
+	}
+out_skip:
+	if (rx->wr_ptr == RX_CMD_COUNT || (rx->wr_ptr == 0 && rd_ptr == 0)) {
+		rx->wr_ptr = 0;
+		if (client)
+			aspeed_mctp_rx_trigger(rx);
+		else
+			WRITE_ONCE(rx->rd_ptr, 0);
+	}
+	wake_up_all(&priv->wait_queue);
+
+out_put:
+	if (client)
+		aspeed_mctp_client_put(client);
+}
+
+static void aspeed_mctp_rx_chan_init(struct mctp_channel *rx)
+{
+	struct aspeed_mctp *priv = container_of(rx, typeof(*priv), rx);
+	u32 *rx_cmd = (u32 *)rx->cmd.vaddr;
+	u32 rx_data_addr = rx->data.dma_handle;
+	struct mctp_pcie_packet packet;
+	u32 data_size = sizeof(packet.data);
+	u32 i;
+
+	for (i = 0; i < RX_CMD_COUNT; i++) {
+		*rx_cmd |= RX_DATA_ADDR(rx_data_addr);
+		*rx_cmd |= RX_INTERRUPT_AFTER_CMD;
+		rx_data_addr += data_size;
+		rx_cmd++;
+	}
+	regmap_write(priv->map, ASPEED_MCTP_RX_BUF_SIZE, RX_CMD_COUNT);
+}
+
+static void aspeed_mctp_tx_chan_init(struct mctp_channel *tx)
+{
+	struct aspeed_mctp *priv = container_of(tx, typeof(*priv), tx);
+
+	regmap_write(priv->map, ASPEED_MCTP_TX_BUF_SIZE, TX_CMD_COUNT);
+	regmap_write(priv->map, ASPEED_MCTP_TX_BUF_WR_PTR, 0);
+}
+
+static int aspeed_mctp_open(struct inode *inode, struct file *file)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct aspeed_mctp *priv = platform_get_drvdata(pdev);
+	struct mctp_client *client;
+	int ret;
+
+	if (priv->pcie.bdf == 0) {
+		ret = -ENODEV;
+		goto out;
+	}
+
+	client = aspeed_mctp_client_alloc(priv);
+	if (!client) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	spin_lock_bh(&priv->clients_lock);
+	/* TODO: Add support for multiple clients  */
+	if (!list_empty(&priv->clients)) {
+		ret = -EBUSY;
+		goto out_unlock;
+	}
+	list_add_tail(&client->link, &priv->clients);
+	spin_unlock_bh(&priv->clients_lock);
+
+	/*
+	 * kick the tasklet to trigger rx
+	 * bh_disable/enable is just to make sure that the tasklet gets
+	 * scheduled immediately in process context without any unnecessary
+	 * delay
+	 */
+	local_bh_disable();
+	tasklet_hi_schedule(&priv->rx.tasklet);
+	local_bh_enable();
+
+	file->private_data = client;
+
+	return 0;
+out_unlock:
+	spin_unlock_bh(&priv->clients_lock);
+	aspeed_mctp_client_put(client);
+out:
+	return ret;
+}
+
+static int aspeed_mctp_release(struct inode *inode, struct file *file)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+
+	spin_lock_bh(&priv->clients_lock);
+	list_del(&client->link);
+	spin_unlock_bh(&priv->clients_lock);
+
+	/* Disable the tasklet to appease lockdep */
+	local_bh_disable();
+	aspeed_mctp_client_put(client);
+	local_bh_enable();
+
+	return 0;
+}
+
+static ssize_t aspeed_mctp_read(struct file *file, char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_pcie_packet *rx_packet;
+	size_t packet_sz = sizeof(rx_packet->data);
+
+	if (READ_ONCE(client->disconnected))
+		return -EIO;
+
+	if (buf && count > 0) {
+		if (count > packet_sz)
+			count = packet_sz;
+
+		rx_packet = ptr_ring_consume_bh(&client->rx_queue);
+		if (!rx_packet)
+			return -EAGAIN;
+
+		if (copy_to_user(buf, &rx_packet->data, count)) {
+			dev_err(priv->dev, "copy to user failed\n");
+			packet_free(rx_packet);
+			return -EFAULT;
+		}
+
+		packet_free(rx_packet);
+	}
+
+	return count;
+}
+
+static ssize_t aspeed_mctp_write(struct file *file, const char __user *buf,
+				 size_t count, loff_t *ppos)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	struct mctp_pcie_packet *tx_packet;
+	int ret;
+
+	if (READ_ONCE(client->disconnected))
+		return -EIO;
+
+	tx_packet = packet_alloc(GFP_KERNEL);
+	if (!tx_packet) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (buf && count > 0) {
+		if (count > sizeof(tx_packet->data)) {
+			ret = -ENOSPC;
+			goto out_packet;
+		}
+
+		if (copy_from_user(&tx_packet->data, buf, count)) {
+			dev_err(priv->dev, "copy from user failed\n");
+			ret = -EFAULT;
+			goto out_packet;
+		}
+		tx_packet->size = count;
+
+		ret = ptr_ring_produce_bh(&client->tx_queue, tx_packet);
+		if (ret)
+			goto out_packet;
+
+		tasklet_hi_schedule(&priv->tx.tasklet);
+	}
+
+	return count;
+
+out_packet:
+	packet_free(tx_packet);
+out:
+	return ret;
+}
+
+static int
+aspeed_mctp_filter_eid(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_filter_eid eid;
+
+	if (copy_from_user(&eid, userbuf, sizeof(eid))) {
+		dev_err(priv->dev, "copy from user failed\n");
+		return -EFAULT;
+	}
+
+	if (eid.enable) {
+		regmap_write(priv->map, ASPEED_MCTP_EID, eid.eid);
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL,
+				   MATCHING_EID, MATCHING_EID);
+	} else {
+		regmap_update_bits(priv->map, ASPEED_MCTP_CTRL,
+				   MATCHING_EID, 0);
+	}
+	return 0;
+}
+
+static int aspeed_mctp_get_bdf(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_bdf bdf = { priv->pcie.bdf };
+
+	if (copy_to_user(userbuf, &bdf, sizeof(bdf))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static int
+aspeed_mctp_get_medium_id(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_medium_id id = { 0x09 }; /* PCIe revision 2.0 */
+
+	if (copy_to_user(userbuf, &id, sizeof(id))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static int
+aspeed_mctp_get_mtu(struct aspeed_mctp *priv, void __user *userbuf)
+{
+	struct aspeed_mctp_get_mtu id = { ASPEED_MCTP_MTU };
+
+	if (copy_to_user(userbuf, &id, sizeof(id))) {
+		dev_err(priv->dev, "copy to user failed\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static long
+aspeed_mctp_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	void __user *userbuf = (void __user *)arg;
+	int ret;
+
+	switch (cmd) {
+	case ASPEED_MCTP_IOCTL_FILTER_EID:
+		ret = aspeed_mctp_filter_eid(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_BDF:
+		ret = aspeed_mctp_get_bdf(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_MEDIUM_ID:
+		ret = aspeed_mctp_get_medium_id(priv, userbuf);
+	break;
+
+	case ASPEED_MCTP_IOCTL_GET_MTU:
+		ret = aspeed_mctp_get_mtu(priv, userbuf);
+	break;
+
+	default:
+		dev_err(priv->dev, "Command not found\n");
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static __poll_t aspeed_mctp_poll(struct file *file,
+				 struct poll_table_struct *pt)
+{
+	struct mctp_client *client = file->private_data;
+	struct aspeed_mctp *priv = client->priv;
+	__poll_t ret = 0;
+
+	poll_wait(file, &priv->wait_queue, pt);
+
+	if (!ptr_ring_full_bh(&client->tx_queue))
+		ret |= EPOLLOUT;
+
+	if (__ptr_ring_peek(&client->rx_queue))
+		ret |= EPOLLIN;
+
+	return ret;
+}
+
+static const struct file_operations aspeed_mctp_fops = {
+	.owner = THIS_MODULE,
+	.open = aspeed_mctp_open,
+	.release = aspeed_mctp_release,
+	.read = aspeed_mctp_read,
+	.write = aspeed_mctp_write,
+	.unlocked_ioctl = aspeed_mctp_ioctl,
+	.poll = aspeed_mctp_poll,
+};
+
+static const struct regmap_config aspeed_mctp_regmap_cfg = {
+	.reg_bits	= 32,
+	.reg_stride	= 4,
+	.val_bits	= 32,
+	.max_register	= ASPEED_MCTP_TX_BUF_WR_PTR,
+};
+
+struct device_type aspeed_mctp_type = {
+	.name		= "aspeed-mctp",
+};
+
+static struct miscdevice aspeed_mctp_miscdev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "aspeed-mctp",
+	.fops = &aspeed_mctp_fops,
+};
+
+static void aspeed_mctp_send_pcie_uevent(struct kobject *kobj, bool ready)
+{
+	char *pcie_not_ready_event[] = { ASPEED_MCTP_READY "=0", NULL };
+	char *pcie_ready_event[] = { ASPEED_MCTP_READY "=1", NULL };
+
+	kobject_uevent_env(kobj, KOBJ_CHANGE,
+			   ready ? pcie_ready_event : pcie_not_ready_event);
+}
+
+static void aspeed_mctp_pcie_setup(struct aspeed_mctp *priv)
+{
+	u32 reg;
+
+	regmap_read(priv->pcie.map, ASPEED_PCIE_MISC_STS_1, &reg);
+
+	priv->pcie.bdf = PCI_DEVID(GET_PCI_BUS_NUM(reg), GET_PCI_DEV_NUM(reg));
+	if (priv->pcie.bdf != 0)
+		cancel_delayed_work(&priv->pcie.rst_dwork);
+	else
+		schedule_delayed_work(&priv->pcie.rst_dwork,
+				      msecs_to_jiffies(1000));
+}
+
+static void aspeed_mctp_irq_enable(struct aspeed_mctp *priv)
+{
+	u32 enable = TX_CMD_LAST_INT | TX_CMD_WRONG_INT |
+		     RX_CMD_RECEIVE_INT;
+
+	regmap_write(priv->map, ASPEED_MCTP_INT_EN, enable);
+}
+
+static void aspeed_mctp_irq_disable(struct aspeed_mctp *priv)
+{
+	regmap_write(priv->map, ASPEED_MCTP_INT_EN, 0);
+}
+
+static void aspeed_mctp_reset_work(struct work_struct *work)
+{
+	struct aspeed_mctp *priv = container_of(work, typeof(*priv),
+						pcie.rst_dwork.work);
+
+	/*
+	 * Client "disconnection" is permanent to avoid forcing the user to use
+	 * uevents in order to monitor for reset. Even if no reads/writes are
+	 * issued during pci reset, userspace will still get -EIO afterwards,
+	 * forcing it to reopen and check the BDF (which may have potentially
+	 * changed after reset). Uevents can be used to avoid looping in open()
+	 */
+	if (priv->pcie.need_uevent) {
+		struct kobject *kobj = &aspeed_mctp_miscdev.this_device->kobj;
+		struct mctp_client *client;
+
+		spin_lock_bh(&priv->clients_lock);
+		client = list_first_entry_or_null(&priv->clients,
+						  typeof(*client), link);
+		/*
+		 * Here, we're only updating the "disconnected" flag under
+		 * spinlock, meaning that we can skip taking the refcount
+		 */
+		if (client)
+			WRITE_ONCE(client->disconnected, true);
+		spin_unlock_bh(&priv->clients_lock);
+
+		aspeed_mctp_send_pcie_uevent(kobj, false);
+		priv->pcie.need_uevent = false;
+	}
+
+	aspeed_mctp_pcie_setup(priv);
+
+	if (priv->pcie.bdf) {
+		aspeed_mctp_send_pcie_uevent(&priv->dev->kobj, true);
+		aspeed_mctp_irq_enable(priv);
+	}
+}
+
+static void aspeed_mctp_channels_init(struct aspeed_mctp *priv)
+{
+	aspeed_mctp_rx_chan_init(&priv->rx);
+	aspeed_mctp_tx_chan_init(&priv->tx);
+}
+
+static irqreturn_t aspeed_mctp_irq_handler(int irq, void *arg)
+{
+	struct aspeed_mctp *priv = arg;
+	u32 handled = 0;
+	u32 status;
+
+	regmap_read(priv->map, ASPEED_MCTP_INT_STS, &status);
+	regmap_write(priv->map, ASPEED_MCTP_INT_STS, status);
+
+	if (status & TX_CMD_LAST_INT) {
+		u32 rd_ptr;
+
+		regmap_write(priv->map, ASPEED_MCTP_TX_BUF_RD_PTR,
+			     UPDATE_RX_RD_PTR);
+		regmap_read(priv->map, ASPEED_MCTP_TX_BUF_RD_PTR, &rd_ptr);
+		rd_ptr &= TX_BUFFER_RD_PTR;
+
+		/*
+		 * rd_ptr on TX side seems to be busted...
+		 * Since we're always reading zeroes, let's trust that when
+		 * we're getting LAST_CMD irq, everything we previously
+		 * submitted was transmitted and start from 0
+		 */
+		WRITE_ONCE(priv->tx.rd_ptr, TX_CMD_COUNT);
+
+		tasklet_hi_schedule(&priv->tx.tasklet);
+
+		wake_up_all(&priv->wait_queue);
+
+		handled |= TX_CMD_LAST_INT;
+	}
+
+	if (status & TX_CMD_WRONG_INT) {
+		/* TODO: print the actual command */
+		dev_warn(priv->dev, "TX wrong");
+
+		handled |= TX_CMD_WRONG_INT;
+	}
+
+	if (status & RX_CMD_RECEIVE_INT) {
+		u32 rd_ptr;
+
+		regmap_write(priv->map, ASPEED_MCTP_RX_BUF_RD_PTR,
+			     UPDATE_RX_RD_PTR);
+		regmap_read(priv->map, ASPEED_MCTP_RX_BUF_RD_PTR, &rd_ptr);
+		rd_ptr &= RX_BUFFER_RD_PTR;
+
+		WRITE_ONCE(priv->rx.rd_ptr, rd_ptr > 0 ? rd_ptr : RX_CMD_COUNT);
+
+		tasklet_hi_schedule(&priv->rx.tasklet);
+
+		handled |= RX_CMD_RECEIVE_INT;
+	}
+
+	if (!handled)
+		return IRQ_NONE;
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t aspeed_mctp_pcie_rst_irq_handler(int irq, void *arg)
+{
+	struct aspeed_mctp *priv = arg;
+
+	aspeed_mctp_channels_init(priv);
+
+	priv->pcie.need_uevent = true;
+	priv->pcie.bdf = 0;
+
+	schedule_delayed_work(&priv->pcie.rst_dwork, 0);
+
+	return IRQ_HANDLED;
+}
+
+static void aspeed_mctp_drv_init(struct aspeed_mctp *priv)
+{
+	init_waitqueue_head(&priv->wait_queue);
+	INIT_LIST_HEAD(&priv->clients);
+
+	spin_lock_init(&priv->clients_lock);
+
+	INIT_DELAYED_WORK(&priv->pcie.rst_dwork, aspeed_mctp_reset_work);
+
+	tasklet_init(&priv->tx.tasklet, aspeed_mctp_tx_tasklet,
+		     (unsigned long)&priv->tx);
+	tasklet_init(&priv->rx.tasklet, aspeed_mctp_rx_tasklet,
+		     (unsigned long)&priv->rx);
+}
+
+static void aspeed_mctp_drv_fini(struct aspeed_mctp *priv)
+{
+	tasklet_disable(&priv->tx.tasklet);
+	tasklet_kill(&priv->tx.tasklet);
+	tasklet_disable(&priv->rx.tasklet);
+	tasklet_kill(&priv->rx.tasklet);
+
+	cancel_delayed_work_sync(&priv->pcie.rst_dwork);
+}
+
+static int aspeed_mctp_resources_init(struct aspeed_mctp *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+	void __iomem *regs;
+
+	regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(regs)) {
+		dev_err(priv->dev, "Failed to get regmap!\n");
+		return PTR_ERR(regs);
+	}
+
+	priv->map = devm_regmap_init_mmio(priv->dev, regs,
+					  &aspeed_mctp_regmap_cfg);
+	if (IS_ERR(priv->map))
+		return PTR_ERR(priv->map);
+
+	priv->reset = devm_reset_control_get(priv->dev, 0);
+	if (IS_ERR(priv->reset)) {
+		dev_err(priv->dev, "Failed to get reset!\n");
+		return PTR_ERR(priv->reset);
+	}
+
+	priv->pcie.map =
+		syscon_regmap_lookup_by_phandle(priv->dev->of_node,
+						"aspeed,pcieh");
+	if (IS_ERR(priv->pcie.map)) {
+		dev_err(priv->dev, "Failed to find PCIe Host regmap!\n");
+		return PTR_ERR(priv->pcie.map);
+	}
+
+	platform_set_drvdata(pdev, priv);
+
+	return 0;
+}
+
+static int aspeed_mctp_dma_init(struct aspeed_mctp *priv)
+{
+	struct mctp_channel *tx = &priv->tx;
+	struct mctp_channel *rx = &priv->rx;
+	int ret = -ENOMEM;
+
+	BUILD_BUG_ON(TX_CMD_COUNT >= TX_MAX_CMD_COUNT);
+	BUILD_BUG_ON(RX_CMD_COUNT >= RX_MAX_CMD_COUNT);
+
+	tx->cmd.vaddr = dma_alloc_coherent(priv->dev, TX_CMD_BUF_SIZE,
+					   &tx->cmd.dma_handle, GFP_KERNEL);
+
+	if (!tx->cmd.vaddr)
+		return ret;
+
+	tx->data.vaddr = dma_alloc_coherent(priv->dev, TX_DATA_BUF_SIZE,
+					    &tx->data.dma_handle, GFP_KERNEL);
+
+	if (!tx->data.vaddr)
+		goto out_tx_data;
+
+	rx->cmd.vaddr = dma_alloc_coherent(priv->dev, RX_CMD_BUF_SIZE,
+					   &rx->cmd.dma_handle, GFP_KERNEL);
+
+	if (!rx->cmd.vaddr)
+		goto out_tx_cmd;
+
+	rx->data.vaddr = dma_alloc_coherent(priv->dev, RX_DATA_BUF_SIZE,
+					    &rx->data.dma_handle, GFP_KERNEL);
+
+	if (!rx->data.vaddr)
+		goto out_rx_data;
+
+	return 0;
+out_rx_data:
+	dma_free_coherent(priv->dev, RX_CMD_BUF_SIZE, rx->cmd.vaddr,
+			  rx->cmd.dma_handle);
+
+out_tx_cmd:
+	dma_free_coherent(priv->dev, TX_DATA_BUF_SIZE, tx->data.vaddr,
+			  tx->data.dma_handle);
+
+out_tx_data:
+	dma_free_coherent(priv->dev, TX_CMD_BUF_SIZE, tx->cmd.vaddr,
+			  tx->cmd.dma_handle);
+	return ret;
+}
+
+static void aspeed_mctp_dma_fini(struct aspeed_mctp *priv)
+{
+	struct mctp_channel *tx = &priv->tx;
+	struct mctp_channel *rx = &priv->rx;
+
+	dma_free_coherent(priv->dev, TX_CMD_BUF_SIZE, tx->cmd.vaddr,
+			  tx->cmd.dma_handle);
+
+	dma_free_coherent(priv->dev, RX_CMD_BUF_SIZE, rx->cmd.vaddr,
+			  rx->cmd.dma_handle);
+
+	dma_free_coherent(priv->dev, TX_DATA_BUF_SIZE, tx->data.vaddr,
+			  tx->data.dma_handle);
+
+	dma_free_coherent(priv->dev, RX_DATA_BUF_SIZE, rx->data.vaddr,
+			  rx->data.dma_handle);
+}
+
+static int aspeed_mctp_irq_init(struct aspeed_mctp *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+	int irq, ret;
+
+	irq = platform_get_irq(pdev, 0);
+	if (!irq)
+		return -ENODEV;
+
+	ret = devm_request_irq(priv->dev, irq, aspeed_mctp_irq_handler,
+			       IRQF_SHARED, "aspeed-mctp", priv);
+	if (ret)
+		return ret;
+
+	irq = platform_get_irq(pdev, 1);
+	if (!irq)
+		return -ENODEV;
+
+	ret = devm_request_irq(priv->dev, irq,
+			       aspeed_mctp_pcie_rst_irq_handler,
+			       IRQF_SHARED, "aspeed-mctp", priv);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void aspeed_mctp_hw_reset(struct aspeed_mctp *priv)
+{
+	if (reset_control_assert(priv->reset) != 0)
+		dev_warn(priv->dev, "Failed to assert reset\n");
+
+	if (reset_control_deassert(priv->reset) != 0)
+		dev_warn(priv->dev, "Failed to deassert reset\n");
+}
+
+static int aspeed_mctp_probe(struct platform_device *pdev)
+{
+	struct aspeed_mctp *priv;
+	int ret;
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	priv->dev = &pdev->dev;
+
+	aspeed_mctp_drv_init(priv);
+
+	ret = aspeed_mctp_resources_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init resources\n");
+		goto out_drv;
+	}
+
+	ret = aspeed_mctp_dma_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init DMA\n");
+		goto out_drv;
+	}
+
+	/*
+	 * FIXME: We need to verify how to make the reset work when we probe
+	 * multiple times. Currently calling reset more than once seems to make
+	 * the HW upset, however, we do need to reset after boot before we're
+	 * able to use the HW.
+	 */
+	aspeed_mctp_hw_reset(priv);
+
+	aspeed_mctp_channels_init(priv);
+
+	aspeed_mctp_miscdev.parent = priv->dev;
+	ret = misc_register(&aspeed_mctp_miscdev);
+	if (ret) {
+		dev_err(priv->dev, "Failed to register miscdev\n");
+		goto out_dma;
+	}
+	aspeed_mctp_miscdev.this_device->type = &aspeed_mctp_type;
+
+	ret = aspeed_mctp_irq_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init IRQ!\n");
+		goto out_dma;
+	}
+
+	aspeed_mctp_irq_enable(priv);
+
+	aspeed_mctp_pcie_setup(priv);
+
+	return 0;
+
+out_dma:
+	aspeed_mctp_dma_fini(priv);
+out_drv:
+	aspeed_mctp_drv_fini(priv);
+out:
+	dev_err(priv->dev, "Failed to probe Aspeed MCTP: %d\n", ret);
+	return ret;
+}
+
+static int aspeed_mctp_remove(struct platform_device *pdev)
+{
+	struct aspeed_mctp *priv = platform_get_drvdata(pdev);
+
+	misc_deregister(&aspeed_mctp_miscdev);
+
+	aspeed_mctp_irq_disable(priv);
+
+	aspeed_mctp_dma_fini(priv);
+
+	aspeed_mctp_drv_fini(priv);
+
+	return 0;
+}
+
+static const struct of_device_id aspeed_mctp_match_table[] = {
+	{ .compatible = "aspeed,ast2600-mctp" },
+	{ }
+};
+
+static struct platform_driver aspeed_mctp_driver = {
+	.driver	= {
+		.name		= "aspeed-mctp",
+		.of_match_table	= of_match_ptr(aspeed_mctp_match_table),
+	},
+	.probe	= aspeed_mctp_probe,
+	.remove	= aspeed_mctp_remove,
+};
+
+static int __init aspeed_mctp_init(void)
+{
+	packet_cache =
+		kmem_cache_create_usercopy("mctp-packet",
+					   sizeof(struct mctp_pcie_packet),
+					   0, 0, 0,
+					   sizeof(struct mctp_pcie_packet),
+					   NULL);
+	if (!packet_cache)
+		return -ENOMEM;
+
+	return platform_driver_register(&aspeed_mctp_driver);
+}
+
+static void __exit aspeed_mctp_exit(void)
+{
+	platform_driver_unregister(&aspeed_mctp_driver);
+	kmem_cache_destroy(packet_cache);
+}
+
+module_init(aspeed_mctp_init)
+module_exit(aspeed_mctp_exit)
+
+MODULE_DEVICE_TABLE(of, aspeed_mctp_match_table);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Iwona Winiarska <iwona.winiarska@intel.com>");
+MODULE_DESCRIPTION("Aspeed AST2600 MCTP driver");
\ No newline at end of file
diff --git a/drivers/misc/npcm-vdm/Kconfig b/drivers/misc/npcm-vdm/Kconfig
index a2c3845acc37..2c200b05836a 100644
--- a/drivers/misc/npcm-vdm/Kconfig
+++ b/drivers/misc/npcm-vdm/Kconfig
@@ -3,7 +3,7 @@
 #
 menu "Nuvoton Miscellaneous optional drivers"
 
-config NPCM_VDM
+config NPCM7XX_VDM
 	tristate "support npcmx50 VDM on PCIe"
 	depends on ARCH_NPCM7XX
 	default y
diff --git a/drivers/misc/npcm7xx-pci-vdm.c b/drivers/misc/npcm7xx-pci-vdm.c
new file mode 100644
index 000000000000..032cf848815a
--- /dev/null
+++ b/drivers/misc/npcm7xx-pci-vdm.c
@@ -0,0 +1,781 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (c) 2014-2018 Nuvoton Technology corporation.
+
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/gpio.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/clk.h>
+#include <linux/uaccess.h>
+#include <linux/regmap.h>
+#include <linux/mfd/syscon.h>
+#include <linux/cdev.h>
+#include <linux/miscdevice.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/kfifo.h>
+#include <linux/poll.h>
+#include <linux/ptr_ring.h>
+
+#define ENABLE_VDMA 1
+
+#define DEVICE_NAME	"npcm7xx-vdm"
+#define TX_TIMEOUT			2000
+
+/* GCR  Register */
+#define MFSEL3 0x64
+#define  MFSEL3_PCIE_PUSE BIT(17)
+#define INTCR3 0x9C
+#define  INTCR3_PCIRREL BIT(30)
+
+/* VDM  Register */
+#define VDMX_BA 0xE0800000
+#define VDMX_STATR 0x0000
+#define  VDMX_STATR_RXNDW   GENMASK(23, 16)
+#define  VDMX_STATR_RXNDW_OFFSET    16
+#define  VDMX_STATR_RXDR BIT(2)
+#define  VDMX_STATR_RXF BIT(1)
+#define  VDMX_STATR_TXS BIT(0)
+#define VDMX_IEN 0x0004
+#define  VDMX_IEN_RXDREN BIT(2)
+#define  VDMX_IEN_RXFEN BIT(1)
+#define  VDMX_IEN_TXSEN BIT(0)
+#define VDMX_RXF 0x0008
+#define VDMX_TXF 0x000C
+#define VDMX_CNT 0x0010
+#define  VDMX_CNT_RXTO_05US BIT(4)
+#define  VDMX_CNT_RXTO_1US  BIT(5)
+#define  VDMX_CNT_RXTO_2US  (BIT(5) | BIT(4))
+#define  VDMX_CNT_RXTO_4US  BIT(6)
+#define  VDMX_CNT_RXTO_8US  (BIT(6) | BIT(4))
+#define  VDMX_CNT_VDMXEN    BIT(1)
+#define  VDMX_CNT_TXP   BIT(0)
+#define VDMX_RXFILT	0x0014
+#define  VDMX_RXFILT_FEN    BIT(31)
+#define  VDMX_VENDOR_ID     0xb4a1
+
+#define VDMX_RX_LEN	4
+#define VDMX_TX_LEN	4
+#define VDMA_BUFFER_SIZE SZ_16K
+
+/* VDMA  Register */
+#define VDMA_CTL 	0x0000
+#define  VDMA_CTL_BLOCK_BIT_POS BIT(17)
+#define  VDMA_CTL_BME BIT(9)
+#define  VDMA_CTL_VDMAEN BIT(0)
+
+#define VDMA_SRCB	0x0004
+#define VDMA_DSTB	0x0008
+#define VDMA_CDST	0x0014
+#define VDMA_CTCNT	0x0018
+#define VDMA_ECTL	0x0040
+#define  VDMA_ECTL_DRDYEN   BIT(29)
+#define  VDMA_ECTL_DRDY BIT(28)
+#define  VDMA_ECTL_NRTGIEN  BIT(27)
+#define  VDMA_ECTL_NORTG  BIT(26)
+#define  VDMA_ECTL_HALTINTEN  BIT(25)
+#define  VDMA_ECTL_DMAHALT  BIT(24)
+#define  VDMA_ECTL_BUFFSIZE_POS 16
+#define  VDMA_ECTL_RTRGREQ  BIT(12)
+#define  VDMA_ECTL_RTRGSZ 	BIT(11)
+#define  VDMA_ECTL_ENSTSUP1 	BIT(10)
+#define  VDMA_ECTL_ENSTSUP0 	BIT(9)
+#define  VDMA_ECTL_SZOFS_POS 7
+#define  VDMA_ECTL_SZMOD_BIT23_16 BIT(5) | BIT(4)
+#define  VDMA_ECTL_STAMPP BIT(2)
+#define  VDMA_ECTL_CYCBUF BIT(1)
+
+#define VDMA_ESRCSZ	0x0044
+#define VDMA_ERDPNT	0x0048
+#define VDMA_EST0AD	0x0050
+#define VDMA_EST0MK	0x0054
+#define VDMA_EST0DT	0x0058
+#define VDMA_EST1AD	0x0060
+#define VDMA_EST1MK	0x0064
+#define VDMA_EST1DT	0x0068
+
+#define VDMA_TX_DONE BIT(0)
+#define VDMA_RX_DONE BIT(1)
+#define VDMA_RX_FULL BIT(2)
+
+typedef void *(*copy_func_t)(void *dest, const void *src, size_t n);
+
+struct mctp_pcie_packet {
+	struct {
+		u32 hdr[4];
+		u32 payload[16];
+	} data;
+	u32 size;
+};
+
+struct npcm7xx_vdm {
+	struct device *dev;
+	struct miscdevice miscdev;
+	spinlock_t lock;
+	int is_open;
+	int irq;
+	struct regmap *vdmx_base;
+	struct regmap *vdma_base;
+	dma_addr_t  dma;
+	void *virt;
+	struct regmap *gcr_base;
+	wait_queue_head_t	wait_queue;
+	struct tasklet_struct tasklet;
+	struct ptr_ring rx_queue;
+	u32 wrap;
+};
+
+static atomic_t npcm7xx_vdm_open_count = ATOMIC_INIT(0);
+
+struct kmem_cache *packet_cache;
+
+static void *packet_alloc(gfp_t flags)
+{
+	return kmem_cache_alloc(packet_cache, flags);
+}
+
+static void packet_free(void *packet)
+{
+	kmem_cache_free(packet_cache, packet);
+}
+
+static void npcm7xx_vdm_rx_tasklet(unsigned long data)
+{
+	struct npcm7xx_vdm *priv = (struct npcm7xx_vdm *)data;
+	struct mctp_pcie_packet *rx_packet;
+	int ret;
+	u32 rdpnt = 0;
+	u32 curdst = 0;
+	u32 size = 0;
+
+	regmap_read(priv->vdma_base, VDMA_CDST, &curdst);
+	regmap_read(priv->vdma_base, VDMA_ERDPNT, &rdpnt);
+
+	pr_info("curdst 0x%x \n", curdst);
+	pr_info("rdpnt 0x%x \n", rdpnt);
+	pr_info("priv->virt 0x%x \n", priv->virt);
+	pr_info("priv->dma 0x%x \n", priv->dma);
+#if 0
+	/* todo: handle wrapped around*/
+	if (curdst < rdpnt) {
+		u32 free = 0;
+		size = (priv->dma + VDMA_BUFFER_SIZE) - rdpnt;
+
+		rx_packet = packet_alloc(GFP_ATOMIC);
+		if (!rx_packet) {
+			dev_err(priv->dev, "Failed to allocate RX packet\n");
+			goto out_skip;
+		}
+	
+		if (size <= sizeof(rx_packet->data)){
+			memcpy(&rx_packet->data,
+				(void *)(priv->virt + (rdpnt - priv->dma)),
+				);
+			rdpnt = priv->dma;
+
+			free = sizeof(rx_packet->data) - size;
+			if (free >= (curdst - priv->dma))
+				memcpy(&rx_packet->data[size],
+				(void *)priv->virt),
+				(curdst - priv->dma));
+			rdpnt = curdst;
+			goto out_skip;
+	}
+#endif
+	// handle wrapped around
+	while (rdpnt > curdst) {
+		
+		priv->wrap++;
+
+		rx_packet = packet_alloc(GFP_ATOMIC);
+		if (!rx_packet) {
+			dev_err(priv->dev, "Failed to allocate RX packet\n");
+			goto out_skip;
+		}
+		
+		size = (priv->dma + VDMA_BUFFER_SIZE) - rdpnt;
+
+		if (size > sizeof(rx_packet->data))
+			size = sizeof(rx_packet->data);
+
+		pr_info("rdpnt size 0x%x \n", size);
+
+		memcpy(&rx_packet->data,
+			(void *)(priv->virt + (rdpnt - priv->dma)),
+		size);
+	
+		rx_packet->size = size;
+		
+		rdpnt += size;
+
+		ret = ptr_ring_produce(&priv->rx_queue, rx_packet);
+		if (ret) {
+			dev_warn(priv->dev, "Failed to produce RX packet: %d\n",
+				ret);
+			packet_free(rx_packet);
+			continue;
+		}
+
+		if (rdpnt == (priv->dma + VDMA_BUFFER_SIZE)) {
+			int left = curdst - priv->dma;
+			int free = sizeof(rx_packet->data) - size;
+
+			if (left <= free) {
+				memcpy(((void *)&rx_packet->data) + size, (void *)(priv->virt), left);
+				rx_packet->size += left;
+				rdpnt = priv->dma + left;
+			} else {
+				rdpnt = priv->dma;
+			}
+			break;
+		}
+ 	}
+
+	while (rdpnt < curdst) {
+
+		rx_packet = packet_alloc(GFP_ATOMIC);
+		if (!rx_packet) {
+			dev_err(priv->dev, "Failed to allocate RX packet\n");
+			goto out_skip;
+		}
+		
+		size = curdst - rdpnt;
+		if (size > sizeof(rx_packet->data))
+			size = sizeof(rx_packet->data);
+
+		pr_info("size 0x%x \n", size);
+
+		memcpy(&rx_packet->data,
+			(void *)(priv->virt + (rdpnt - priv->dma)),
+			size);
+	
+		rx_packet->size = size;
+
+		rdpnt += size;
+
+		ret = ptr_ring_produce(&priv->rx_queue, rx_packet);
+		if (ret) {
+			dev_warn(priv->dev, "Failed to produce RX packet: %d\n",
+				ret);
+			packet_free(rx_packet);
+			continue;
+		}
+	}
+
+out_skip:
+	pr_info("priv->wrap %d \n", priv->wrap);
+	pr_info("update rdpnt 0x%x \n", rdpnt);
+	regmap_write(priv->vdma_base, VDMA_ERDPNT, rdpnt);
+
+	wake_up_all(&priv->wait_queue);
+}
+
+static irqreturn_t npcm7xx_vdm_irq(int irq, void *arg)
+{
+	struct npcm7xx_vdm *priv = arg;
+	u32 status = 0;
+
+	regmap_read(priv->vdma_base, VDMA_ECTL, &status);
+	pr_info("status 0x%x \n", status);
+
+	if (status & VDMA_ECTL_DRDY) {
+		regmap_update_bits(priv->vdma_base, VDMA_ECTL, VDMA_ECTL_DRDY, VDMA_ECTL_DRDY);
+		tasklet_hi_schedule(&priv->tasklet);
+	}
+
+	if (status & VDMA_ECTL_DMAHALT) {
+		/*to do: we should reinit dam buffer if dma halt*/
+		regmap_update_bits(priv->vdma_base, VDMA_ECTL, VDMA_ECTL_DMAHALT, VDMA_ECTL_DMAHALT);
+		pr_info("VDMA_ECTL_DMAHALT \n");
+	}	
+
+	if (status & VDMA_ECTL_NORTG) {
+		regmap_update_bits(priv->vdma_base, VDMA_ECTL, VDMA_ECTL_NORTG, VDMA_ECTL_NORTG);
+		pr_info("VDMA_ECTL_NORTG \n");
+	}	
+
+	return IRQ_HANDLED;
+}
+
+static void npcm7xx_vdm_hw_finit(struct npcm7xx_vdm *priv)
+{
+	/* VDM RESET */
+	regmap_write(priv->vdmx_base, VDMX_CNT, 0);
+
+	/* Disable Interrupt */
+	regmap_write(priv->vdmx_base, VDMX_IEN, 0);
+
+	/* Clear VDM STAT */
+	regmap_write(priv->vdmx_base, VDMX_STATR,
+		  VDMX_STATR_RXDR | VDMX_STATR_RXF | VDMX_STATR_TXS);
+
+	/* VDMA Disable */
+	regmap_write(priv->vdma_base, VDMA_CTL, 0);
+
+	/* VDMA Disable Interrupt */
+	regmap_update_bits(priv->vdma_base, VDMA_ECTL, VDMA_ECTL_DRDYEN, ~VDMA_ECTL_DRDYEN);
+	regmap_update_bits(priv->vdma_base, VDMA_ECTL, VDMA_ECTL_HALTINTEN, ~VDMA_ECTL_HALTINTEN);
+	regmap_update_bits(priv->vdma_base, VDMA_ECTL, VDMA_ECTL_NRTGIEN, ~VDMA_ECTL_NRTGIEN);
+
+	/* Clear VDMA State*/
+	regmap_write(priv->vdma_base, VDMA_ECTL,
+		  VDMA_ECTL_DRDY | VDMA_ECTL_NORTG | VDMA_ECTL_DMAHALT);
+
+}
+
+static int npcm7xx_vdm_hw_init(struct npcm7xx_vdm *priv)
+{
+	int ret = 0;
+	u32 reg_val = 0;
+	struct regmap *gcr = priv->gcr_base;
+
+	/* Configure PCIE phy selection for bridge */
+	regmap_update_bits(gcr, MFSEL3, MFSEL3_PCIE_PUSE, ~MFSEL3_PCIE_PUSE);
+
+	/* Clear VDM STAT */
+	regmap_write(priv->vdmx_base, VDMX_STATR,
+		  VDMX_STATR_RXDR | VDMX_STATR_RXF | VDMX_STATR_TXS);
+
+	/* VDM RESET */
+	regmap_write(priv->vdmx_base, VDMX_CNT, 0);
+	reg_val = VDMX_CNT_RXTO_8US | VDMX_CNT_VDMXEN;
+	regmap_write(priv->vdmx_base, VDMX_CNT, reg_val);
+
+	/* VDM Filter */
+	reg_val = 0;//VDMX_RXFILT_FEN | VDMX_VENDOR_ID;
+	regmap_write(priv->vdmx_base, VDMX_RXFILT, reg_val);
+
+	/* Enable RX int */
+	regmap_write(priv->vdmx_base, VDMX_IEN, VDMX_IEN_RXFEN);
+
+	/* VDMA Disable */
+	regmap_write(priv->vdma_base, VDMA_CTL, 0);
+
+	regmap_write(priv->vdma_base, VDMA_SRCB, VDMX_BA | VDMX_RXF); // src_addr
+	regmap_write(priv->vdma_base, VDMA_DSTB, priv->dma); // dst_addr
+
+	regmap_write(priv->vdma_base, VDMA_ESRCSZ, VDMX_BA | VDMX_STATR);	// size_addr
+	regmap_write(priv->vdma_base, VDMA_ERDPNT, priv->dma);	// read pointer
+
+	regmap_write(priv->vdma_base, VDMA_EST0AD, VDMX_BA | VDMX_STATR);	// address of clear bit
+	regmap_write(priv->vdma_base, VDMA_EST0MK, 0xffffffff);
+	regmap_write(priv->vdma_base, VDMA_EST0DT, VDMX_IEN_RXFEN);
+
+	reg_val = VDMA_ECTL_DRDYEN | VDMA_ECTL_DRDY | VDMA_ECTL_HALTINTEN | VDMA_ECTL_DMAHALT |
+		VDMA_ECTL_RTRGSZ | VDMA_ECTL_ENSTSUP0 | VDMA_ECTL_CYCBUF | VDMA_ECTL_SZMOD_BIT23_16 |
+		((VDMA_BUFFER_SIZE / SZ_16K) << VDMA_ECTL_BUFFSIZE_POS);
+	regmap_write(priv->vdma_base, VDMA_ECTL, reg_val);
+
+	reg_val = VDMA_CTL_BLOCK_BIT_POS | VDMA_CTL_VDMAEN;
+	regmap_write(priv->vdma_base, VDMA_CTL, reg_val);
+
+	return ret;
+}
+
+static ssize_t npcm7xx_vdm_read(struct file *file, char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct npcm7xx_vdm *priv = platform_get_drvdata(pdev);
+	struct mctp_pcie_packet *rx_packet;
+
+	if (buf && count > 0) {
+		rx_packet = ptr_ring_consume_bh(&priv->rx_queue);
+		if (!rx_packet)
+			return -EAGAIN;
+
+		if (count > rx_packet->size)
+			count = rx_packet->size;
+
+		if (copy_to_user(buf, &rx_packet->data, count)) {
+			dev_err(priv->dev, "copy to user failed\n");
+			packet_free(rx_packet);
+			return -EFAULT;
+		}
+
+		packet_free(rx_packet);
+	}
+
+	return count;
+}
+
+static int
+npcm7xx_vdm_send(struct npcm7xx_vdm *priv, u8 *txbuf, int size)
+{
+	u32 timeout, stat;
+	int i, ret = -EIO;
+
+	regmap_read(priv->vdmx_base, VDMX_CNT, &stat);
+	if (stat & VDMX_CNT_VDMXEN) {
+		ret = size;
+
+		for (i = 0; i < size; i += VDMX_TX_LEN) {
+			pr_info("VDMX_TXF 0x%x \n", readl(txbuf + i));
+			regmap_write(priv->vdmx_base, VDMX_TXF, readl(txbuf + i));
+		}
+
+		regmap_update_bits(priv->vdmx_base, VDMX_CNT, VDMX_CNT_TXP, VDMX_CNT_TXP);
+
+		timeout = wait_event_interruptible_timeout(priv->wait_queue,
+						!regmap_read(priv->vdmx_base, VDMX_CNT, &stat) &
+							!(stat & VDMX_CNT_TXP),
+							msecs_to_jiffies(TX_TIMEOUT));
+		if (!timeout) {
+			pr_info("npcm7xx_vdm_send timeout tx_stat 0x%x\n", stat);
+			ret = -EIO;
+		}
+
+		/**/
+		regmap_update_bits(priv->vdmx_base, VDMX_STATR, VDMX_STATR_TXS, VDMX_STATR_TXS);
+	}
+	
+	return ret;
+}
+
+static ssize_t npcm7xx_vdm_write(struct file *file, const char __user *buf,
+				 size_t count, loff_t *ppos)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct npcm7xx_vdm *priv = platform_get_drvdata(pdev);
+	struct mctp_pcie_packet *tx_packet;
+	int ret;
+
+	if (!access_ok(buf, count)) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	tx_packet = packet_alloc(GFP_KERNEL);
+	if (!tx_packet) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (buf && count > 0) {
+		if (count > sizeof(tx_packet->data)) {
+			ret = -ENOSPC;
+			goto out_packet;
+		}
+
+		if (copy_from_user(&tx_packet->data, buf, count)) {
+			dev_err(priv->dev, "copy from user failed\n");
+			ret = -EFAULT;
+			goto out_packet;
+		}
+		tx_packet->size = count;
+
+		ret = npcm7xx_vdm_send(priv, (u8 *)&tx_packet->data, tx_packet->size);
+	}
+
+out_packet:
+	packet_free(tx_packet);
+out:
+	return ret;
+}
+
+static int npcm7xx_vdm_open(struct inode *inode, struct file *file)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct npcm7xx_vdm *priv = platform_get_drvdata(pdev);
+
+	if (atomic_inc_return(&npcm7xx_vdm_open_count) == 1) {
+		if (npcm7xx_vdm_hw_init(priv)) {
+			pr_info("Failed to init VDM\n");
+			return -EBUSY;;
+		}
+
+		local_bh_disable();
+		tasklet_hi_schedule(&priv->tasklet);
+		local_bh_enable();
+
+		return 0;
+	}
+
+	atomic_dec(&npcm7xx_vdm_open_count);
+	return -EBUSY;
+}
+
+static int npcm7xx_vdm_release(struct inode *inode, struct file *file)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct npcm7xx_vdm *priv = platform_get_drvdata(pdev);
+
+	if (atomic_dec_return(&npcm7xx_vdm_open_count) == 0)
+		npcm7xx_vdm_hw_finit(priv);
+
+	return 0;
+}
+
+
+static __poll_t npcm7xx_vdm_poll(struct file *file,
+				 struct poll_table_struct *pt)
+{
+	struct miscdevice *misc = file->private_data;
+	struct platform_device *pdev = to_platform_device(misc->parent);
+	struct npcm7xx_vdm *priv = platform_get_drvdata(pdev);
+
+	__poll_t ret = 0;
+
+	poll_wait(file, &priv->wait_queue, pt);
+
+	if (__ptr_ring_peek(&priv->rx_queue))
+		ret |= EPOLLIN;
+
+	return ret;
+}
+
+const struct file_operations npcm7xx_vdm_fops = {
+	.llseek = noop_llseek,
+	.open = npcm7xx_vdm_open,
+	.read = npcm7xx_vdm_read,
+	.write = npcm7xx_vdm_write,
+	.release = npcm7xx_vdm_release,
+	.poll = npcm7xx_vdm_poll,
+};
+
+static int npcm7xx_vdm_config_irq(struct npcm7xx_vdm *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+	int irq, ret;
+
+	irq = platform_get_irq(pdev, 0);
+	if (!irq)
+		return -ENODEV;
+
+	ret = devm_request_irq(priv->dev, irq, npcm7xx_vdm_irq, IRQF_SHARED, DEVICE_NAME, priv);
+	if (ret < 0) {
+		dev_err(priv->dev, "Unable to request IRQ %d\n", irq);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void npcm7xx_vdm_tsk_init(struct npcm7xx_vdm *priv)
+{
+	spin_lock_init(&priv->lock);
+	init_waitqueue_head(&priv->wait_queue);
+
+	ptr_ring_init(&priv->rx_queue, VDMX_RX_LEN, GFP_ATOMIC);
+
+	tasklet_init(&priv->tasklet, npcm7xx_vdm_rx_tasklet,
+		     (unsigned long)priv);
+}
+
+static void npcm7xx_vdm_tsk_fini(struct npcm7xx_vdm *priv)
+{
+	tasklet_disable(&priv->tasklet);
+	tasklet_kill(&priv->tasklet);
+}
+
+static const struct regmap_config npcm7xx_vdmx_regmap_cfg = {
+       .reg_bits       = 32,
+       .reg_stride     = 4,
+       .val_bits       = 32,
+       .max_register   = VDMX_RXFILT,
+};
+
+static const struct regmap_config npcm7xx_vdma_regmap_cfg = {
+       .reg_bits       = 32,
+       .reg_stride     = 4,
+       .val_bits       = 32,
+       .max_register   = VDMA_EST1DT,
+};
+
+static int npcm7xx_vdm_resources_init(struct npcm7xx_vdm *priv)
+{
+	struct platform_device *pdev = to_platform_device(priv->dev);
+
+	void __iomem *regs;
+
+	regs = devm_platform_ioremap_resource(pdev, 0);
+	if (IS_ERR(regs)) {
+		dev_err(priv->dev, "Failed to get regmap!\n");
+		return PTR_ERR(regs);
+	}
+
+	priv->vdmx_base = devm_regmap_init_mmio(priv->dev, regs,
+					  &npcm7xx_vdmx_regmap_cfg);
+	if (IS_ERR(priv->vdmx_base))
+		return PTR_ERR(priv->vdmx_base);
+	
+
+	regs = devm_platform_ioremap_resource(pdev, 1);
+	if (IS_ERR(regs)) {
+		dev_err(priv->dev, "Failed to get regmap!\n");
+		return PTR_ERR(regs);
+	}
+
+	priv->vdma_base = devm_regmap_init_mmio(priv->dev, regs,
+					  &npcm7xx_vdma_regmap_cfg);
+	if (IS_ERR(priv->vdma_base))
+		return PTR_ERR(priv->vdma_base);
+
+	priv->gcr_base =
+		syscon_regmap_lookup_by_compatible("nuvoton,npcm750-gcr");
+	if (IS_ERR(priv->gcr_base)) {
+		dev_err(priv->dev, "failed to find nuvoton,npcm750-gcr\n");
+		return PTR_ERR(priv->gcr_base);
+	}
+
+	platform_set_drvdata(pdev, priv);
+
+	return 0;
+}
+
+static int npcm7xx_vdm_dma_init(struct npcm7xx_vdm *priv)
+{
+	priv->virt = dma_alloc_coherent(priv->dev, VDMA_BUFFER_SIZE,
+					&priv->dma, GFP_KERNEL);
+	if (!priv->virt) {
+		dev_err(priv->dev, "Failed to allocate DMA for VDMA \n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void npcm7xx_vdm_dma_fini(struct npcm7xx_vdm *priv)
+{
+	dma_free_coherent(priv->dev, VDMA_BUFFER_SIZE,
+			priv->virt, priv->dma);
+}
+
+struct device_type npcm7xx_vdm_type = {
+	.name		= DEVICE_NAME,
+};
+
+static struct miscdevice npcm7xx_vdm_miscdev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = DEVICE_NAME,
+	.fops = &npcm7xx_vdm_fops,
+};
+
+
+static int npcm_vdm_probe(struct platform_device *pdev)
+{
+	struct npcm7xx_vdm *priv;
+	int ret;
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	priv->dev = &pdev->dev;
+	dev_set_drvdata(&pdev->dev, priv);
+
+	ret = npcm7xx_vdm_resources_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init resources\n");
+		goto out_drv;
+	}
+
+	ret = npcm7xx_vdm_dma_init(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to init DMA\n");
+		goto out_drv;
+	}
+
+	/* register miscdev */
+	npcm7xx_vdm_miscdev.parent = priv->dev;
+	ret = misc_register(&npcm7xx_vdm_miscdev);
+	if (ret) {
+		dev_err(priv->dev, "Failed to register miscdev\n");
+		goto out_dma;
+	}
+	npcm7xx_vdm_miscdev.this_device->type = &npcm7xx_vdm_type;
+
+	npcm7xx_vdm_hw_finit(priv);
+
+	npcm7xx_vdm_tsk_init(priv);
+
+	ret = npcm7xx_vdm_config_irq(priv);
+	if (ret) {
+		dev_err(priv->dev, "Failed to configure IRQ\n");
+		goto out_dma;
+	}
+
+	pr_info("NPCM7XX VDM Driver probed\n");
+
+	return 0;
+
+out_dma:
+	npcm7xx_vdm_dma_fini(priv);
+out_drv:
+	npcm7xx_vdm_tsk_fini(priv);
+out:
+	dev_err(priv->dev, "Failed to probe Nuvoton VDM: %d\n", ret);
+	return ret;
+}
+
+static int npcm_vdm_remove(struct platform_device *pdev)
+{
+	struct npcm7xx_vdm *priv = platform_get_drvdata(pdev);
+
+	if (!priv)
+		return 0;
+
+	misc_deregister(&npcm7xx_vdm_miscdev);
+
+	ptr_ring_cleanup(&priv->rx_queue, &packet_free);
+
+	npcm7xx_vdm_hw_finit(priv);
+
+ 	npcm7xx_vdm_dma_fini(priv);
+
+	npcm7xx_vdm_tsk_fini(priv);
+
+ 	kfree(priv);
+
+	return 0;
+}
+
+static const struct of_device_id npcm7xx_vdm_match[] = {
+	{ .compatible = "nuvoton,npcm750-vdm", },
+	{},
+};
+
+static struct platform_driver npcm_vdm_driver = {
+	.probe          = npcm_vdm_probe,
+	.remove			= npcm_vdm_remove,
+	.driver         = {
+		.name   = DEVICE_NAME,
+		.owner	= THIS_MODULE,
+		.of_match_table = npcm7xx_vdm_match,
+	},
+};
+
+static int __init npcm_vdm_init(void)
+{
+	packet_cache =
+		kmem_cache_create_usercopy("mctp-packet",
+					   sizeof(struct mctp_pcie_packet),
+					   0, 0, 0,
+					   sizeof(struct mctp_pcie_packet),
+					   NULL);
+	if (!packet_cache)
+		return -ENOMEM;
+
+	return platform_driver_register(&npcm_vdm_driver);
+}
+
+static void __exit npcm_vdm_exit(void)
+{
+	platform_driver_unregister(&npcm_vdm_driver);
+	kmem_cache_destroy(packet_cache);
+}
+
+module_init(npcm_vdm_init)
+module_exit(npcm_vdm_exit)
+
+MODULE_DEVICE_TABLE(of, npcm7xx_vdm_match);
+MODULE_AUTHOR("Nuvoton Technology Corp.");
+MODULE_DESCRIPTION("VDM Master Driver");
+MODULE_LICENSE("GPL");
-- 
2.17.1

